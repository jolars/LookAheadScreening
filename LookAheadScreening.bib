
@inproceedings{bonnefoy2014,
  title = {A Dynamic Screening Principle for the Lasso},
  booktitle = {{{EUSIPCO}} 2014},
  author = {Bonnefoy, Antoine and Emiya, Valentin and Ralaivola, Liva and Gribonval, R{\'e}mi},
  year = {2014},
  month = sep,
  pages = {6--10},
  publisher = {{IEEE}},
  address = {{Lisbon, Portugal}},
  abstract = {The Lasso is an optimization problem devoted to finding a sparse representation of some signal with respect to a predefined dictionary. An original and computationally-efficient method is proposed here to solve this problem, based on a dynamic screening principle. It makes it possible to accelerate a large class of optimization algorithms by iteratively reducing the size of the dictionary during the optimization process, discarding elements that are provably known not to belong to the solution of the Lasso. The iterative reduction of the dictionary is what we call dynamic screening. As this screening step is inexpensive, the computational cost of the algorithm using our dynamic screening strategy is lower than that of the base algorithm. Numerical experiments on synthetic and real data support the relevance of this approach.},
  isbn = {978-0-9928626-1-9}
}
% == BibTeX quality report for bonnefoy2014:
% ? Unsure about the formatting of the booktitle

@article{bonnefoy2015,
  title = {Dynamic Screening: Accelerating First-Order Algorithms for the Lasso and Group-Lasso},
  shorttitle = {Dynamic {{Screening}}},
  author = {Bonnefoy, Antoine and Emiya, Valentin and Ralaivola, Liva and Gribonval, Remi},
  year = {2015},
  month = oct,
  volume = {63},
  pages = {5121--5132},
  issn = {1053-587X, 1941-0476},
  doi = {10.1109/TSP.2015.2447503},
  abstract = {Recent computational strategies based on screening tests have been proposed to accelerate algorithms addressing penalized sparse regression problems such as the Lasso. Such approaches build upon the idea that it is worth dedicating some small computational effort to locate inactive atoms and remove them from the dictionary in a preprocessing stage so that the regression algorithm working with a smaller dictionary will then converge faster to the solution of the initial problem. We believe that there is an even more efficient way to screen the dictionary and obtain a greater acceleration: inside each iteration of the regression algorithm, one may take advantage of the algorithm computations to obtain a new screening test for free with increasing screening effects along the iterations. The dictionary is henceforth dynamically screened instead of being screened statically, once and for all, before the first iteration. We formalize this dynamic screening principle in a general algorithmic scheme and apply it by embedding inside a number of first-order algorithms adapted existing screening tests to solve the Lasso or new screening tests to solve the Group-Lasso. Computational gains are assessed in a large set of experiments on synthetic data as well as real-world sounds and images. They show both the screening efficiency and the gain in terms running times.},
  journal = {IEEE Transactions on Signal Processing},
  language = {en},
  number = {19}
}

@book{boyd2004,
  title = {Convex Optimization},
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  year = {2004},
  edition = {First},
  publisher = {{Cambridge University Press}},
  address = {{New York, NY, USA}},
  isbn = {978-0-521-83378-3}
}

@techreport{elghaoui2010,
  title = {Safe Feature Elimination in Sparse Supervised Learning},
  author = {El Ghaoui, Laurent and Viallon, Vivian and Rabbani, Tarek},
  year = {2010},
  month = sep,
  address = {{Berkeley}},
  institution = {{EECS Department, University of California}},
  abstract = {We investigate fast methods that allow to quickly eliminate variables (features) in supervised learning problems involving a convex loss function and a l{$_1$}-norm penalty, leading to a potentially substantial reduction in the number of variables prior to running the supervised learning algorithm. The methods are not heuristic: they only eliminate features that are \emph{guaranteed} to be absent after solving the learning problem. Our framework applies to a large class of problems, including support vector machine classification, logistic regression and least-squares. The complexity of the feature elimination step is negligible compared to the typical computational effort involved in the sparse supervised learning problem: it grows linearly with the number of features times the number of examples, with much better count if data is sparse. We apply our method to data sets arising in text classification and observe a dramatic reduction of the dimensionality, hence in computational effort required to solve the learning problem, especially when very sparse classifiers are sought. Our method allows to immediately extend the scope of existing algorithms, allowing us to run them on data sets of sizes that were out of their reach before.},
  number = {UCB/EECS-2010-126}
}

@inproceedings{fercoq2015,
  title = {Mind the Duality Gap: Safer Rules for the Lasso},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Fercoq, Olivier and Gramfort, Alexandre and Salmon, Joseph},
  editor = {Bach, Francis and Blei, David},
  year = {2015},
  month = jul,
  volume = {37},
  pages = {333--342},
  publisher = {{PMLR}},
  address = {{Lille, France}},
  abstract = {Screening rules allow to early discard irrelevant variables from the optimization in Lasso problems, or its derivatives, making solvers faster. In this paper, we propose new versions of the so-called rules for the Lasso. Based on duality gap considerations, our new rules create safe test regions whose diameters converge to zero, provided that one relies on a converging solver. This property helps screening out more variables, for a wider range of regularization parameter values. In addition to faster convergence, we prove that we correctly identify the active sets (supports) of the solutions in finite time. While our proposed strategy can cope with any solver, its performance is demonstrated using a coordinate descent algorithm particularly adapted to machine learning use cases. Significant computing time reductions are obtained with respect to previous safe rules.},
  series = {Proceedings of Machine Learning Research}
}
% == BibTeX quality report for fercoq2015:
% ? Unsure about the formatting of the booktitle

@article{friedman2007,
  title = {Pathwise Coordinate Optimization},
  author = {Friedman, Jerome and Hastie, Trevor and H{\"o}fling, Holger and Tibshirani, Robert},
  year = {2007},
  month = dec,
  volume = {1},
  pages = {302--332},
  issn = {1932-6157},
  doi = {10/d88g8c},
  abstract = {We consider ``one-at-a-time'' coordinate-wise descent algorithms for a class of convex optimization problems. An algorithm of this kind has been proposed for the L1-penalized regression (lasso) in the literature, but it seems to have been largely ignored. Indeed, it seems that coordinate-wise algorithms are not often used in convex optimization. We show that this algorithm is very competitive with the well-known LARS (or homotopy) procedure in large lasso problems, and that it can be applied to related methods such as the garotte and elastic net. It turns out that coordinate-wise descent does not work in the ``fused lasso,'' however, so we derive a generalized algorithm that yields the solution in much less time that a standard convex optimizer. Finally, we generalize the procedure to the two-dimensional fused lasso, and demonstrate its performance on some image smoothing problems.},
  journal = {The Annals of Applied Statistics},
  language = {EN},
  number = {2}
}

@article{friedman2010,
  title = {Regularization Paths for Generalized Linear Models via Coordinate Descent},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  year = {2010},
  volume = {33},
  pages = {1--22},
  doi = {10.18637/jss.v033.i01},
  journal = {Journal of Statistical Software},
  number = {1}
}

@inproceedings{johnson2015,
  title = {Blitz: A Principled Meta-Algorithm for Scaling Sparse Optimization},
  booktitle = {Proceedings  of  the 32nd {{International}}  {{Conference}}  on  {{Machine Learning}}},
  author = {Johnson, Tyler B and Guestrin, Carlos},
  year = {2015},
  volume = {37},
  pages = {9},
  publisher = {{JMLR: W\&CP}},
  address = {{Lille, France}},
  abstract = {By reducing optimization to a sequence of small subproblems, working set methods achieve fast convergence times for many challenging problems. Despite excellent performance, theoretical understanding of working sets is limited, and implementations often resort to heuristics to determine subproblem size, makeup, and stopping criteria. We propose BLITZ, a fast working set algorithm accompanied by useful guarantees. Making no assumptions on data, our theory relates subproblem size to progress toward convergence. This result motivates methods for optimizing algorithmic parameters and discarding irrelevant variables as iterations progress. Applied to 1-regularized learning, BLITZ convincingly outperforms existing solvers in sequential, limited-memory, and distributed settings. BLITZ is not specific to 1-regularized learning, making the algorithm relevant to many applications involving sparsity or constraints.},
  language = {en}
}

@inproceedings{massias2018,
  title = {Celer: A Fast Solver for the Lasso with Dual Extrapolation},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Massias, Mathurin and Gramfort, Alexandre and Salmon, Joseph},
  editor = {Dy, Jennifer and Krause, Andreas},
  year = {2018},
  month = jul,
  volume = {80},
  pages = {3315--3324},
  publisher = {{PMLR}},
  address = {{Stockholm, Sweden}},
  abstract = {Convex sparsity-inducing regularizations are ubiquitous in high-dimensional machine learning, but solving the resulting optimization problems can be slow. To accelerate solvers, state-of-the-art approaches consist in reducing the size of the optimization problem at hand. In the context of regression, this can be achieved either by discarding irrelevant features (screening techniques) or by prioritizing features likely to be included in the support of the solution (working set techniques). Duality comes into play at several steps in these techniques. Here, we propose an extrapolation technique starting from a sequence of iterates in the dual that leads to the construction of improved dual points. This enables a tighter control of optimality as used in stopping criterion, as well as better screening performance of Gap Safe rules. Finally, we propose a working set strategy based on an aggressive use of Gap Safe screening rules. Thanks to our new dual point construction, we show significant computational speedups on multiple real-world problems.},
  language = {en},
  series = {Proceedings of {{Machine Learning Research}}}
}

@article{ndiaye2017,
  title = {Gap Safe Screening Rules for Sparsity Enforcing Penalties},
  author = {Ndiaye, Eugene and Fercoq, Olivier and Gramfort, Alexandre and Salmon, Joseph},
  year = {2017},
  volume = {18},
  pages = {1--33},
  journal = {Journal of Machine Learning Research},
  number = {128}
}

@article{osborne2000,
  title = {On the {{LASSO}} and Its Dual},
  author = {Osborne, Michael R. and Presnell, Brett and Turlach, Berwin A.},
  year = {2000},
  volume = {9},
  pages = {319--337},
  issn = {1061-8600},
  doi = {10.2307/1390657},
  abstract = {Proposed by Tibshirani, the least absolute shrinkage and selection operator (LASSO) estimates a vector of regression coefficients by minimizing the residual sum of squares subject to a constraint on the l\textsuperscript{1}-norm of the coefficient vector. The LASSO estimator typically has one or more zero elements and thus shares characteristics of both shrinkage estimation and variable selection. In this article we treat the LASSO as a convex programming problem and derive its dual. Consideration of the primal and dual problems together leads to important new insights into the characteristics of the LASSO estimator and to an improved method for estimating its covariance matrix. Using these results we also develop an efficient algorithm for computing LASSO estimates which is usable even in cases where the number of regressors exceeds the number of observations. An S-Plus library based on this algorithm is available from StatLib.},
  journal = {Journal of Computational and Graphical Statistics},
  number = {2}
}

@article{tibshirani1996,
  title = {Regression Shrinkage and Selection via the Lasso},
  author = {Tibshirani, Robert},
  year = {1996},
  volume = {58},
  pages = {267--288},
  issn = {0035-9246},
  abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  number = {1}
}

@article{tibshirani2012,
  title = {Strong Rules for Discarding Predictors in Lasso-Type Problems},
  author = {Tibshirani, Robert and Bien, Jacob and Friedman, Jerome and Hastie, Trevor and Simon, Noah and Taylor, Jonathan and Tibshirani, Ryan J.},
  year = {2012},
  month = mar,
  volume = {74},
  pages = {245--266},
  issn = {1369-7412},
  doi = {10/c4bb85},
  journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
  language = {English},
  number = {2}
}
% == BibTeX quality report for tibshirani2012:
% ? Possibly abbreviated journal title Journal of the Royal Statistical Society. Series B: Statistical Methodology

@inproceedings{wang2014,
  title = {A Safe Screening Rule for Sparse Logistic Regression},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 1},
  author = {Wang, Jie and Zhou, Jiayu and Liu, Jun and Wonka, Peter and Ye, Jieping},
  year = {2014},
  month = dec,
  pages = {1053--1061},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  abstract = {The {$\mathscr{l}$}1-regularized logistic regression (or sparse logistic regression) is a widely used method for simultaneous classification and feature selection. Although many recent efforts have been devoted to its efficient implementation, its application to high dimensional data still poses significant challenges. In this paper, we present a fast and effective sparse logistic regression screening rule (Slores) to identify the "0" components in the solution vector, which may lead to a substantial reduction in the number of features to be entered to the optimization. An appealing feature of Slores is that the data set needs to be scanned only once to run the screening and its computational cost is negligible compared to that of solving the sparse logistic regression problem. Moreover, Slores is independent of solvers for sparse logistic regression, thus Slores can be integrated with any existing solver to improve the efficiency. We have evaluated Slores using high-dimensional data sets from different applications. Experiments demonstrate that Slores outperforms the existing state-of-the-art screening rules and the efficiency of solving sparse logistic regression can be improved by one magnitude.},
  series = {{{NIPS}}'14}
}

@article{wang2015,
  title = {Lasso Screening Rules via Dual Polytope Projection},
  author = {Wang, Jie and Wonka, Peter and Ye, Jieping},
  year = {2015},
  month = may,
  volume = {16},
  pages = {1063--1101},
  issn = {1532-4435},
  abstract = {Lasso is a widely used regression technique to find sparse representations. When the dimension of the feature space and the number of samples are extremely large, solving the Lasso problem remains challenging. To improve the efficiency of solving large-scale Lasso problems, El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify the inactive predictors, i.e., predictors that have 0 components in the solution vector. Then, the inactive predictors or features can be removed from the optimization problem to reduce its scale. By transforming the standard Lasso to its dual form, it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution. In this paper, we propose an efficient and effective screening rule via Dual Polytope Projections (DPP), which is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope. Moreover, we show that our screening rule can be extended to identify inactive groups in group Lasso. To the best of our knowledge, there is currently no exact screening rule for group Lasso. We have evaluated our screening rule using synthetic and real data sets. Results show that our rule is more effective in identifying inactive predictors than existing state-of-the-art screening rules for Lasso.},
  journal = {Journal of Machine Learning Research},
  number = {1}
}

@article{wu2008,
  title = {Coordinate Descent Algorithms for Lasso Penalized Regression},
  author = {Wu, Tong Tong and Lange, Kenneth},
  year = {2008},
  month = mar,
  volume = {2},
  pages = {224--244},
  issn = {1932-6157, 1941-7330},
  doi = {10/dqhmdg},
  abstract = {Imposition of a lasso penalty shrinks parameter estimates toward zero and performs continuous model selection. Lasso penalized regression is capable of handling linear regression problems where the number of predictors far exceeds the number of cases. This paper tests two exceptionally fast algorithms for estimating regression coefficients with a lasso penalty. The previously known {$\mathscr{l}$}2 algorithm is based on cyclic coordinate descent. Our new {$\mathscr{l}$}1 algorithm is based on greedy coordinate descent and Edgeworth's algorithm for ordinary {$\mathscr{l}$}1 regression. Each algorithm relies on a tuning constant that can be chosen by cross-validation. In some regression problems it is natural to group parameters and penalize parameters group by group rather than separately. If the group penalty is proportional to the Euclidean norm of the parameters of the group, then it is possible to majorize the norm and reduce parameter estimation to {$\mathscr{l}$}2 regression with a lasso penalty. Thus, the existing algorithm can be extended to novel settings. Each of the algorithms discussed is tested via either simulated or real data or both. The Appendix proves that a greedy form of the {$\mathscr{l}$}2 algorithm converges to the minimum value of the objective function.},
  journal = {The Annals of Applied Statistics},
  language = {EN},
  mrnumber = {MR2415601},
  number = {1},
  zmnumber = {1137.62045}
}

@article{zeng2017,
  title = {Efficient Feature Screening for Lasso-Type Problems via Hybrid Safe-Strong {{Rules}}},
  author = {Zeng, Yaohui and Yang, Tianbao and Breheny, Patrick},
  year = {2017},
  month = nov,
  abstract = {The lasso model has been widely used for model selection in data mining, machine learning, and high-dimensional statistical analysis. However, due to the ultrahigh-dimensional, large-scale data sets collected in many real-world applications, it remains challenging to solve the lasso problems even with state-of-the-art algorithms. Feature screening is a powerful technique for addressing the Big Data challenge by discarding inactive features from the lasso optimization. In this paper, we propose a family of hybrid safe-strong rules (HSSR) which incorporate safe screening rules into the sequential strong rule (SSR) to remove unnecessary computational burden. In particular, we present two instances of HSSR, namely SSR-Dome and SSR-BEDPP, for the standard lasso problem. We further extend SSR-BEDPP to the elastic net and group lasso problems to demonstrate the generalizability of the hybrid screening idea. Extensive numerical experiments with synthetic and real data sets are conducted for both the standard lasso and the group lasso problems. Results show that our proposed hybrid rules substantially outperform existing state-of-the-art rules.},
  archiveprefix = {arXiv},
  eprint = {1704.08742},
  eprinttype = {arxiv},
  journal = {arXiv:1704.08742 [stat]},
  primaryclass = {stat}
}
% == BibTeX quality report for zeng2017:
% ? Possibly abbreviated journal title arXiv:1704.08742 [stat]

@article{zeng2021,
  title = {Hybrid Safe\textendash Strong Rules for Efficient Optimization in Lasso-Type Problems},
  author = {Zeng, Yaohui and Yang, Tianbao and Breheny, Patrick},
  year = {2021},
  month = jan,
  volume = {153},
  pages = {107063},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2020.107063},
  abstract = {The lasso model has been widely used for model selection in data mining, machine learning, and high-dimensional statistical analysis. However, with the ultrahigh-dimensional, large-scale data sets now collected in many real-world applications, it is important to develop algorithms to solve the lasso that efficiently scale up to problems of this size. Discarding features from certain steps of the algorithm is a powerful technique for increasing efficiency and addressing the Big Data challenge. This paper proposes a family of hybrid safe\textendash strong rules (HSSR) which incorporate safe screening rules into the sequential strong rule (SSR) to remove unnecessary computational burden. Two instances of HSSR are presented, SSR-Dome and SSR-BEDPP, for the standard lasso problem. SSR-BEDPP is further extended to the elastic net and group lasso problems to demonstrate the generalizability of the hybrid screening idea. Extensive numerical experiments with synthetic and real data sets are conducted for both the standard lasso and the group lasso problems. Results show that the proposed hybrid rules can substantially outperform existing state-of-the-art rules.},
  journal = {Computational Statistics \& Data Analysis},
  language = {en}
}


